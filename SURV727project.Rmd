# SURV727-Project
---
title: "A Sentiment Analysis of Tweets on Georgia's Abortion Ban"
subtitle: "SURV727 Term Paper"
author: "Akipu Ehoche and Ellena Sempeles"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    df_print: kable
---

```{r, include = FALSE}
library(knitr)
library(tidyverse)
library(rtweet)
library(tidytext)
```
## Introduction
Trying to write something new.
Ellena is now trying to write something new.
Third time is the charm
One more time.

#Still checking this out

#ES is trying to add to this file!!uuuuuugh
This section outlines the research idea. We can also cite related work here [@Wickham2014; @Baumer2017].

Note that compiled term paper (the PDF) is supposed to be more text-centered than the RMarkdown documents we used in class, i.e. the text sections are more detailed and big or redundant code chunks can be hidden.

## Data
#checking for error
This section describes the data sources and the data gathering process.


```{r}
# A code chunk that exemplifies the data gathering process
ban_tweets <- stream_tweets(
  q = "georgia ban abortion",
  timeout = 15600,
  file_name = "tweet1",
  parse = FALSE
)
ban_tweets <- stream_tweets(
  q = "georgia ban abortion",
  timeout = 15600,
  file_name = "tweet2",
  parse = FALSE
)
ban_tweets <- stream_tweets(
  q = "georgia ban abortion",
  timeout = 15600,
  file_name = "tweet3",
  parse = FALSE
)
ban_tweets <- stream_tweets(
  q = "georgia ban abortion",
  timeout = 15600,
  file_name = "tweet4",
  parse = FALSE
)


```

```{r, include = FALSE}
# Additional code chunks that repeat tasks or do basic things can be hidden
keywords <- parse_stream("keywords.json")

keywords$text <- gsub("http.*", "", keywords$text)
keywords$text <- gsub("https.*", "", keywords$text)
keywords$text <- gsub("&amp;", "&", keywords$text)

keywords_clean <- keywords %>%
  select(text) %>%
  unnest_tokens(word, text)

nrow(keywords_clean)

stopwds <- get_stopwords("en")
keywords_cleaner <- keywords_clean %>%
  anti_join(stopwds)

nrow(keywords_cleaner)

keywords_cleaner %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n)) +
  coord_flip()
```

## Results

This section presents the main results.

### Data exploration

The results section may have a data exploration part, but in general the structure here depends on the specific project.

```{r}
# What happens here depends on the specific project
```

```{r}
# What happens here depends on the specific project
```

### Analysis

This section presents the main results, such as (for example) stats and graphs that show relationships, model results and/or clustering, PCA, etc.

```{r}
# What happens here depends on the specific project
```

```{r}
# What happens here depends on the specific project
```

```{r}
# What happens here depends on the specific project
```

## Discussion

This section summarizes the results and may briefly outline advantages and limitations of the work presented.
