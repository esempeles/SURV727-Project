---
output:
  word_document: default
  html_document: default
  pdf_document: default
---
# SURV727-Project
---
title: "Sentiment Analysis of Tweets on Georgia Abortion Law"
subtitle: "SURV727 Term Paper"
author: "Akipu Ehoche and Ellena Sempeles"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    df_print: kable
 references:
- id: Blazina2022
  title: Key facts about the abortion debate in America
  author:
  - family: Blazina
    given: C.
  container-title: Pew Research
  type: article-online
  issued:
    year: 2022
- id: BellwareRoubein2022
  title: Judge overturns Georgia’s six-week abortion ban
  author:
  - family: Bellware
    given: K.
  - family: Roubein
    given: R.
  type: article-online
  container-title: The Washington Post
  issued:
    year: 2022
- id: Thanawala2022
  title: Georgia Supreme Court reinstates ban on abortions after 6 weeks of pregnancy
  author:
  - family: Thanawala
    given: S.
  type: article-online
  container-title: PBS
  issued:
    year: 2022
---

```{r, include = FALSE}
library(knitr)
library(tidyverse)
library(rtweet)
library(tidytext)
library(qdap)
library(lubridate)
```
## Introduction
In June of 2022, the Supreme Court issued its decision in Dobbs vs. Jackson Women’s Health, overturning Roe vs. Wade, eliminating the federal standard protecting the right to abortion. This sparked a major political and morality debate across the American population, where opinions and comments were shared via various social media platforms, including Twitter. According to a survey of U.S. adults conducted by Pew Research Center in the summer of 2022, a majority of the U.S. public disapproves of the Supreme Court's decision. Roughly 57% of adults disapprove of the court's decision, including 43% who strongly disapprove, while 41% approve, including 25% who strongly approve [@Blazina2022].The state of Georgia’s abortion law was among of the strictest in the country, banning abortion after the detection of fetal cardiac activity, at roughly six weeks. However, as of November 15th, 2022, a breaking news story reported that a Fulton County judge overturned Georgia’s six-week abortion ban [@BellwareRoubein2022]. Then, as of November 23rd, 2022, the state attorney general's office appealed the ruling to the state Supreme Court, allowing the six-week abortion ban to take effect once again [@Thanawala2022]. Given the country's divide on the issue of abortion and the back and forth of Georgia's legislation, we are interested in the emotional reactions of Twitter users' tweets on these changes of legislation. Our questions include:
1) What are people saying about the changes in Georgia’s abortion law? 
2) Are their reactions positive or negative?
3) Do the sentiment of the tweets change as the legislative events change?

This paper provides a sentiment analysis of Twitter users' tweets on the topic of abortion in the state of Georgia.

## Data
Two corpuses of tweets were collected to examine the sentiment of Twitter users regarding the Georgia abortion ban. Using the Twitter API, a total of 17,415 tweets were collected over two periods of time, after the abortion ban was overturned and after it was reinstated. The first corpus was collected for a span of 12 hours starting at 10:00PM EST on November 15, 2022. Given that the abortion ban was overturning on that specific day, this corpus of tweets captured Twitter users' reaction to the the overturning. This corpus included a total of 3,083 tweets. This corpus will be referred as the "Abortion Ban Overturned" corpus moving forward. The second corpus was collected for a span of 12 hours starting at 3:00PM EST on November 23, 2022. Internet disruptions were experienced while streaming this corpus of tweets, so this corpus was collected in three installments and combined into one corpus. This corpus of tweets captured Twitter users reactions after the news of the ban reinstatement. This corpus included a total of 14,332 tweets. This corpus will be referred to as the "Abortion Ban Reinstated" corpus moving forward.


```{r, eval=FALSE }
## Data Gathering Process
# First corpus of tweets (GA Abortion Law Overturned):
ban_tweets <- stream_tweets(
  q = "georgia ban abortion",
  timeout = 86400,
  file_name = "tweet1",
  parse = FALSE
)

#Second corpus of tweets (GA Abortion Law Reinstated):
ban_tweets <- stream_tweets(
  q = "georgia ban abortion",
  timeout = 1800,
  file_name = "tweet2",
  parse = FALSE
)
ban_tweets <- stream_tweets(
  q = "georgia ban abortion",
  timeout = 43200,
  file_name = "tweet3",
  parse = FALSE
)
ban_tweets <- stream_tweets(
  q = "georgia ban abortion",
  timeout = 15600,
  file_name = "tweet4",
  parse = FALSE
)

```




```{r, warning=FALSE}
# Parse from json file to get a usable dataframe
ovt_tweets = parse_stream("tweet1")
tweets2 = parse_stream("tweet2") 
tweets3 = parse_stream("tweet3")
tweets4 = parse_stream("tweet4")

#Combine GA Abortion Law Reinstated tweets into one corpus:
reinst_tweets = rbind(tweets2, tweets3, tweets4)

#Totals for number of tweets per corpus:
nrow(ovt_tweets)
nrow(reinst_tweets)
```
Next, we investigated the frequency of the tweets over time within each corpus. This would help us better understand the behavior of the Twitter users' as the topic of Georgia's abortion law was discussed. The two plots below show the number of tweets per minute for the Abortion Ban Overturned and Abortion Ban Reinstated corpuses.

**Add language here describing the frequency of tweets per corpus according to the graphs.

```{r, include=TRUE }
ovt_tweets$datetime <- ymd_hms(ovt_tweets$created_at)
ovt_tweets %>% 
  ggplot(aes(datetime)) + 
  geom_freqpoly(binwidth = 60)  # 60 seconds = 1 minute
```
The first corpus of tweets were collected between 9pm and 10am on 15th-16th November 2022.

```{r, include=TRUE}
reinst_tweets$datetime <- ymd_hms(reinst_tweets$created_at)
reinst_tweets %>% 
  ggplot(aes(datetime)) + 
  geom_freqpoly(binwidth = 60)  # 60 seconds = 1 minute
```
The first corpus of tweets were collected between 2pm and 3am on 23rd-24th November 2022. 

Next we did some initial cleaning. This includes removing websites, changing punctuation tags to just punctuation,unnecessary words and stopwords. 
We did this for both of the corpuses.


```{r, include=TRUE}
##Cleaning the Abortion Ban Overturned corpus:
ovt_tweets_keywords <- ovt_tweets
ovt_tweets_keywords$text <- gsub("http.*", "", ovt_tweets_keywords$text)
ovt_tweets_keywords$text <- gsub("https.*", "", ovt_tweets_keywords$text)
ovt_tweets_keywords$text <- gsub("&amp;", "&", ovt_tweets_keywords$text)


ovt_tweets_keywords_clean <- ovt_tweets_keywords %>%
  select(text) %>%
  unnest_tokens(word, text)

nrow(ovt_tweets_keywords_clean)

stopwds <- get_stopwords("en")
ovt_tweets_keywords_cleaner <- ovt_tweets_keywords_clean %>%
  anti_join(stopwds)

nrow(ovt_tweets_keywords_cleaner)
```

```{r, include=TRUE}
##Cleaning the Abortion Ban Reinstated corpus:
reinst_tweets_keywords <- reinst_tweets
reinst_tweets_keywords$text <- gsub("http.*", "", reinst_tweets_keywords$text)
reinst_tweets_keywords$text <- gsub("https.*", "", reinst_tweets_keywords$text)
reinst_tweets_keywords$text <- gsub("&amp;", "&", reinst_tweets_keywords$text)


reinst_tweets_keywords_clean <- reinst_tweets_keywords %>%
  select(text) %>%
  unnest_tokens(word, text)

nrow(reinst_tweets_keywords_clean)

stopwds <- get_stopwords("en")
reinst_tweets_keywords_cleaner <- reinst_tweets_keywords_clean %>%
  anti_join(stopwds)

nrow(reinst_tweets_keywords_cleaner)
```

We then inspected the top keywords for each of the corpuses for further cleaning.
```{r, include=TRUE}
#Plot of Keywords before stemming for Abortion Ban Overturned corpus:
ovt_tweets_plot1 <- ovt_tweets_keywords_cleaner %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n)) +
  coord_flip() 
ovt_tweets_plot1

#Plot of Keywords before stemming for Abortion Ban Reinstated corpus:
reinst_tweets_plot1 <- reinst_tweets_keywords_cleaner %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n)) +
  coord_flip() 
reinst_tweets_plot1

```

Based on these plots, we found that more cleaning should be done prior to sentiment analysis.
For example, additional words/stopwords need to be removed:
just, now, around, week, day, 6, six

Also, we used stemming techniques to further clean the tweets for analyses.
We also removed punctuation, symbols, and numbers (since all tweets will mention the 6-week abortion ban)


Using stemming to clean up the data, we get
```{r, include=TRUE}
library(quanteda)
library(lexicon)

#Additional Cleaning for Abortion Ban Overturned corpus:
ovt_tweets_keywords_cleaner$word <- gsub("just", "", ovt_tweets_keywords_cleaner$word)
ovt_tweets_keywords_cleaner$word <- gsub("now", "", ovt_tweets_keywords_cleaner$word) 
ovt_tweets_keywords_cleaner$word <- gsub("around", "", ovt_tweets_keywords_cleaner$word)
ovt_tweets_keywords_cleaner$word <- gsub("week", "", ovt_tweets_keywords_cleaner$word) 
ovt_tweets_keywords_cleaner$word <- gsub("day", "", ovt_tweets_keywords_cleaner$word)
ovt_tweets_keywords_cleaner$word <- gsub("6", "", ovt_tweets_keywords_cleaner$word)
ovt_tweets_keywords_cleaner$word <- gsub("RT", "", ovt_tweets_keywords_cleaner$word) 
ovt_tweets_keywords_cleaner$word <- gsub("six", "", ovt_tweets_keywords_cleaner$word)


ovt_tweets_keywords_clean_ <- ovt_tweets_keywords_cleaner %>% dplyr::filter(!(word==""))
ovt_tweets_keywords_clean1 <- as.data.frame(ovt_tweets_keywords_clean_)

toks <- corpus(ovt_tweets_keywords_clean1$word) %>%
  tokens(remove_punct = TRUE, remove_numbers = FALSE, remove_symbols = TRUE) %>%  tokens_wordstem()


ovt_tweets_toks1 <- data.frame(text = sapply(toks, as.character), stringsAsFactors = FALSE)
ovt_tweets_plot2 <- ovt_tweets_toks1 %>%  mutate(word=text) %>%
  anti_join(stopwds)%>%
 count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n)) +
  coord_flip()
ovt_tweets_plot2


#Additional Cleaning for Abortion Ban Reinstated corpus:
reinst_tweets_keywords_cleaner$word <- gsub("just", "", reinst_tweets_keywords_cleaner$word)
reinst_tweets_keywords_cleaner$word <- gsub("now", "", reinst_tweets_keywords_cleaner$word)
reinst_tweets_keywords_cleaner$word <- gsub("around", "", reinst_tweets_keywords_cleaner$word) 
reinst_tweets_keywords_cleaner$word <- gsub("week", "", reinst_tweets_keywords_cleaner$word) 
reinst_tweets_keywords_cleaner$word <- gsub("day", "", reinst_tweets_keywords_cleaner$word)
reinst_tweets_keywords_cleaner$word <- gsub("6", "", reinst_tweets_keywords_cleaner$word) 
reinst_tweets_keywords_cleaner$word <- gsub("RT", "", reinst_tweets_keywords_cleaner$word) 
reinst_tweets_keywords_cleaner$word <- gsub("six", "", reinst_tweets_keywords_cleaner$word) 
 

#Removes empty observations
reinst_tweets_keywords_clean_ <- reinst_tweets_keywords_cleaner %>% dplyr::filter(!(word==""))
reinst_tweets_keywords_clean1 <- as.data.frame(reinst_tweets_keywords_clean_)

toksw <-  corpus(reinst_tweets_keywords_clean1$word) %>%
  tokens(remove_punct = TRUE, remove_numbers = FALSE, remove_symbols = TRUE) %>% 
  tokens_wordstem()

reinst_tweets_toks1 <- data.frame(text = sapply(toksw, as.character), stringsAsFactors = FALSE)
reinst_tweets_plot2 <- reinst_tweets_toks1 %>%  mutate(word=text) %>%
  anti_join(stopwds) %>%
  count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n)) +
  coord_flip()
reinst_tweets_plot2

```

Comparing the two plots using the different cleaning methods side by side
```{r, include=TRUE}
library(patchwork)
ovt_tweets_plot1 + ovt_tweets_plot2 + reinst_tweets_plot1 + reinst_tweets_plot2
```




Plot 1 represents cleaning up the data as described above while plot2 has formatted the words after the cleaning using stemming.


```{r, echo=FALSE, mess}
##Sentiment Calculation
library(SentimentAnalysis)

#Sentiment of Abortion Law Overturned corpus:
ovt_tweets_sentiments = analyzeSentiment(iconv(as.character(ovt_tweets_keywords$text), to='UTF-8'))
head(ovt_tweets_sentiments)
#How to get average?

#Sentiment of Abortion Law Reinstated corpus:
reinst_tweets_sentiments = analyzeSentiment(iconv(as.character(reinst_tweets_toks1$text), to='UTF-8'))
head(reinst_tweets_sentiments)
#How to get average?

##Lexicoder
library(quanteda)

#Lexicoder Sentiment of Abortion Law Overturned corpus:
tokenized=quanteda::tokens_lookup(quanteda::tokens(ovt_tweets_toks1$text), dictionary=data_dictionary_LSD2015, exclusive=FALSE)
ovt_tweets_sentiments$LCpos = sapply(tokenized, function(x) sum(x=='POSITIVE') - sum(x=='NEG_POSITIVE') + sum(x=='NEG_NEGATIVE'))
ovt_tweets_sentiments$LCneg = sapply(tokenized, function(x) sum(x=='NEGATIVE') - sum(x=='NEG_NEGATIVE') + sum(x=='NEG_POSITIVE'))
ovt_tweets_sentiments$LC = (sentiments$LCpos-sentiments$LCneg)/sentiments$WordCount

#Lexicoder Sentiment of Abortion Law Reinstated corpus:
tokenized=quanteda::tokens_lookup(quanteda::tokens(reinst_tweets_toks1$text), dictionary=data_dictionary_LSD2015, exclusive=FALSE)
reinst_tweets_sentiments$LCpos = sapply(tokenized, function(x) sum(x=='POSITIVE') - sum(x=='NEG_POSITIVE') + sum(x=='NEG_NEGATIVE'))
reinst_tweets_sentiments$LCneg = sapply(tokenized, function(x) sum(x=='NEGATIVE') - sum(x=='NEG_NEGATIVE') + sum(x=='NEG_POSITIVE'))
reinst_tweets_sentiments$LC = (sentiments$LCpos-sentiments$LCneg)/sentiments$WordCount

##Vader
#install.packages('vader')
library(vader)
#Vader Sentiment of Abortion Law Overturned corpus:
ovt_tweets_vader_scores = vader_df(ovt_tweets_toks1$text)
ovt_tweets_sentiments$Vader = ovt_tweets_vader_scores$compound
#Mean Sentiment:
mean(ovt_tweets_sentiments$Vader, na.rm=TRUE)

#Vader Sentiment of Abortion Law Reinstated corpus:
reinst_tweets_vader_scores = vader_df(reinst_tweets_toks1$text)
reinst_tweets_sentiments$Vader = reinst_tweets_vader_scores$compound
#Mean Sentiment:
mean(reinst_tweets_sentiments$Vader, na.rm=TRUE)


##Sentiment of Abortion Law Overturned vs. Reinstated
par(mfrow=c(1,2))
hist(ovt_tweets_sentiments$Vader, main='Sentiment of Abortion Law Overturned Tweets', xlab='Sentiment')
hist(reinst_tweets_sentiments$Vader, main='Sentiment of Abortion Law Reinstated Tweets', xlab='Sentiment')






#Compare dictionaries
#install.packages('GGally')
library(GGally)
with(sentiments, ggpairs(data.frame(SentimentGI, SentimentHE, SentimentLM, 
                                    SentimentQDAP, LC, Vader)))

```



```{r, echo=FALSE, mess}
#Sentiment Calculation
library(SentimentAnalysis)

sentiments = analyzeSentiment(iconv(as.character(keywords$text), to='UTF-8'))
head(sentiments)

#Lexicoder
library(quanteda)
tokenized=quanteda::tokens_lookup(quanteda::tokens(keywords$text), dictionary=data_dictionary_LSD2015, exclusive=FALSE)
sentiments$LCpos = sapply(tokenized, function(x) sum(x=='POSITIVE') - sum(x=='NEG_POSITIVE') + sum(x=='NEG_NEGATIVE'))
sentiments$LCneg = sapply(tokenized, function(x) sum(x=='NEGATIVE') - sum(x=='NEG_NEGATIVE') + sum(x=='NEG_POSITIVE'))
sentiments$LC = (sentiments$LCpos-sentiments$LCneg)/sentiments$WordCount

#Vader
#install.packages('vader')
library(vader)
vader_scores = vader_df(keywords$text)
sentiments$Vader = vader_scores$compound

#Compare dictionaries
#install.packages('GGally')
library(GGally)
with(sentiments, ggpairs(data.frame(SentimentGI, SentimentHE, SentimentLM, 
                                    SentimentQDAP, LC, Vader)))

```

The second installment will be merged into one json file
```{r, echo = FALSE}
keywords2 <- parse_stream("tweet2")
keywords3 <- parse_stream("tweet3")
keywords4 <- parse_stream("tweet4")
tweet_2 <- rbind(keywords2,keywords3,keywords4)

```


```{r, include=TRUE}
tweet_2$text <- gsub("http.*", "", tweet_2$text)
tweet_2$text <- gsub("https.*", "", tweet_2$text)
tweet_2$text <- gsub("&amp;", "&", tweet_2$text)
tweet_2$text <- gsub("RT", "", tweet_2$text)
tweet_2$text <- gsub("6", "six", tweet_2$text)

keywords_clean <- tweet_2 %>%
  select(text) %>%
  unnest_tokens(word, text)
nrow(keywords_clean)

stopwds <- get_stopwords("en")
keywords_cleaner <- keywords_clean %>%
  anti_join(stopwds)
nrow(keywords_cleaner)

plot3 <- keywords_cleaner %>%
  count(word, sort = TRUE) %>%
  top_n(30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n)) +
  coord_flip() 
plot3
```

```{r, include=TRUE}
library(quanteda)
library(lexicon)

keywords_clean1 <- as.data.frame(keywords_clean)
toks <-  corpus(keywords_clean1$word) %>%
  tokens(remove_punct = TRUE, remove_numbers = FALSE, remove_symbols = TRUE) %>% 
  tokens_wordstem()

toks1 <- data.frame(text = sapply(toks, as.character), stringsAsFactors = FALSE)
plot4 <- toks1 %>%  mutate(word=text) %>%
  anti_join(stopwds) %>%
  count(word, sort = TRUE) %>%
  top_n(30) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(x = word, y = n)) +
  coord_flip()
plot4
```
Comparing all four top 15 words
```{r, include=TRUE}
 plot3 +plot4
```
 
## Results

This section presents the main results.

### Data exploration

The results section may have a data exploration part, but in general the structure here depends on the specific project.

```{r}
# What happens here depends on the specific project
```

```{r}
# What happens here depends on the specific project
```

### Analysis

This section presents the main results, such as (for example) stats and graphs that show relationships, model results and/or clustering, PCA, etc.

```{r}
# What happens here depends on the specific project
```

```{r}
# What happens here depends on the specific project
```

```{r}
# What happens here depends on the specific project
```

## Discussion

This section summarizes the results and may briefly outline advantages and limitations of the work presented.
